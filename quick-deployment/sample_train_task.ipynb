{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6833e5-c8d1-4591-b9ed-af083409ff99",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31e0d6-8b41-4f1b-9c3e-f8433ae51fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from network import build_recurrent_model\n",
    "from load_data import load_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0ee29-3aea-4080-8929-60f0ac3417e1",
   "metadata": {},
   "source": [
    "### Load Experiment Data\n",
    "    - Arguments: \n",
    "        - dataset: {'mnist', 'fashion_mnist', 'imdb', 'cifar10', 'reuters'}\n",
    "        - permute: {True/False}\n",
    "        - orientation: {None, 'post', 'uniform'}\n",
    "\n",
    "    - E.g. 1: load sequential mnist task\n",
    "        -> (train_x, train_y), (test_x, test_y) = load_experiment('mnist', permute=False, orientation=None)\n",
    "\n",
    "    - E.g. 2: load post noise orientation imdb task\n",
    "        -> (train_x, train_y), (test_x, test_y) = load_experiment('imdb', permute=False, orientation='post')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f144fe0-43a2-49dd-abda-0ddcdeb4e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. 1: load sequential mnist task\n",
    "\n",
    "dataset = 'mnist'\n",
    "permute = False\n",
    "orientation = None\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_experiment(dataset, permute=permute, orientation=orientation)\n",
    "\n",
    "print('******** TASK Data Details ********')\n",
    "print(f'dataset: {dataset}')\n",
    "print(f'permute: {permute}')\n",
    "print(f'orientation: {orientation}')\n",
    "\n",
    "print(f'train features dim: {train_x.shape}')\n",
    "print(f'train labels dim: {train_y.shape}')\n",
    "print(f'test features dim: {test_x.shape}')\n",
    "print(f'test labels dim: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f61ecd-ed57-4fd5-9f4a-48c40c588493",
   "metadata": {},
   "source": [
    "### Generate Recurrent Model\n",
    "    - Function: build_recurrent_model(name, time_horizon, ft_dim, hid_dim, output_dim, learning_rate, embed=False)\n",
    "        \n",
    "    - Arguments:\n",
    "            - name (string):          { 'rnn', 'antisymmetric', 'exponential', 'lstm', 'gru', 'lipschitz', 'unicornn' }\n",
    "            - time_horizon (int):     { time horizon of underlying task (i.e., input sequence length) }\n",
    "            - ft_dim (int):           { dimension of input sequence element }\n",
    "            - hid_dim (int):          { recurrrent dimension }\n",
    "            - output_dim (int):       { dimension of output (i.e., number of class labels) }\n",
    "            - learning_rate (float):  { optimizer learning rate }\n",
    "            - embed (bool):           { boolean indicating whether the task requires an embedding layer (NLP tasks) }\n",
    "\n",
    "    - Note: For NLP tasks involving the data sets 'imdb' and 'reuters', a word-embedding is learned. To accomodate this, we fix an embedding dimension to 50 (i.e., set `ft_dim=50` in model initialization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411eeee9-c228-4633-9f99-70a73a75a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "arch = 'rnn'\n",
    "embed = False if dataset in ['mnist', 'fashion_mnist', 'cifar10'] else True\n",
    "T = int(train_x.shape[1])\n",
    "ft_dim = 50 if embed else int(train_x.shape[2]) \n",
    "hid_dim = 128\n",
    "out_dim = 1 if len(np.unique(train_y)) == 2 else len(np.unique(train_y))\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "model = build_recurrent_model(arch, T, ft_dim, hid_dim, out_dim, learning_rate, embed=embed)\n",
    "\n",
    "print('embed:', embed)\n",
    "rec_layer = model.layers[2].name if embed else model.layers[1].name\n",
    "print(f'recurrent layer: {rec_layer}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07e09a-6211-4365-a3c5-7d4d6a30c336",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679a07c-4f75-43b1-84df-39613311ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y, epochs=3, validation_data=(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa9592-99d5-4fb1-b7fe-e9f52f975a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
